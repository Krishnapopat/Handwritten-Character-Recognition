{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10705,"sourceType":"datasetVersion","datasetId":7160},{"sourceId":6148711,"sourceType":"datasetVersion","datasetId":3518586}],"dockerImageVersionId":30587,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport datetime\nimport torch\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom tensorflow.keras import  layers, models \nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport seaborn as sns\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau,ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nimport datetime\nimport numpy as np\nimport seaborn as sns\nimport tensorflow as tf\nimport pickle\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom tensorflow.keras import  layers, models\nfrom sklearn.preprocessing import OneHotEncoder \nfrom keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Dense, Flatten, Dropout\nfrom sklearn.neural_network import MLPClassifier\nfrom keras.optimizers import Adam\nfrom keras.callbacks import TensorBoard\nfrom keras import backend as K\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.models import Model\n\n\ndf = pd.read_csv('/kaggle/input/digit-character/digit_char_dataset.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-20T15:58:25.133202Z","iopub.execute_input":"2023-11-20T15:58:25.133559Z","iopub.status.idle":"2023-11-20T15:59:24.278721Z","shell.execute_reply.started":"2023-11-20T15:58:25.133532Z","shell.execute_reply":"2023-11-20T15:59:24.277290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = pd.read_csv(\"/kaggle/input/emnist/emnist-balanced-train.csv\")\ntest_set = pd.read_csv(\"/kaggle/input/emnist/emnist-balanced-test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-11-20T15:59:24.281085Z","iopub.execute_input":"2023-11-20T15:59:24.281659Z","iopub.status.idle":"2023-11-20T15:59:33.127354Z","shell.execute_reply.started":"2023-11-20T15:59:24.281623Z","shell.execute_reply":"2023-11-20T15:59:33.126167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels1 = train_set[\"45\"].values\nlabels=set(labels1)\nlabels=list(labels)\nlabels = [str(label) for label in labels]\nprint(labels)\n\nplt.figure(figsize=(20,6))\nsns.countplot(x=labels1)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T15:59:33.128681Z","iopub.execute_input":"2023-11-20T15:59:33.129574Z","iopub.status.idle":"2023-11-20T15:59:33.766625Z","shell.execute_reply.started":"2023-11-20T15:59:33.129545Z","shell.execute_reply":"2023-11-20T15:59:33.765259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train = np.array(train_set.iloc[:,0].values)\nX_train = np.array(train_set.iloc[:,1:].values)\n#test_set\nY_test = np.array(test_set.iloc[:,0].values)\nX_test = np.array(test_set.iloc[:,1:].values)\nprint(Y_train.shape)\nprint(X_train.shape)\nX_train= X_train/255\nX_test= X_test/255","metadata":{"execution":{"iopub.status.busy":"2023-11-20T15:59:33.769136Z","iopub.execute_input":"2023-11-20T15:59:33.769512Z","iopub.status.idle":"2023-11-20T15:59:34.273933Z","shell.execute_reply.started":"2023-11-20T15:59:33.769479Z","shell.execute_reply":"2023-11-20T15:59:34.271869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels1 = train_set[\"45\"].values\nlabels=set(labels1)\nlabels=list(labels)\nlabels = [str(label) for label in labels]\nprint(labels)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T15:59:34.275908Z","iopub.execute_input":"2023-11-20T15:59:34.276340Z","iopub.status.idle":"2023-11-20T15:59:34.293015Z","shell.execute_reply.started":"2023-11-20T15:59:34.276285Z","shell.execute_reply":"2023-11-20T15:59:34.291523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train=Y_train.ravel()\nY_test=Y_test.ravel()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T15:59:34.294377Z","iopub.execute_input":"2023-11-20T15:59:34.294684Z","iopub.status.idle":"2023-11-20T15:59:34.303913Z","shell.execute_reply.started":"2023-11-20T15:59:34.294655Z","shell.execute_reply":"2023-11-20T15:59:34.302264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.astype('float32') / 255\nX_test = X_test.astype('float32') / 255\nx_train=X_train\ny_train=Y_train\nx_test=X_test\n# Reshape the data to have a single color channel (since EMNIST is grayscale)\n# and match the input shape expected by the model\nX_train = X_train.reshape((-1, 28, 28, 1))\nX_test = X_test.reshape((-1, 28, 28, 1))\n\n# One-hot encode the target labels for categorical classification.\nY_train = tf.keras.utils.to_categorical(Y_train, 47)  # 37 classes (26 letters + 1 for 'none')\nY_test = tf.keras.utils.to_categorical(Y_test, 47)\nprint(x_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T15:59:34.306354Z","iopub.execute_input":"2023-11-20T15:59:34.306975Z","iopub.status.idle":"2023-11-20T15:59:34.525492Z","shell.execute_reply.started":"2023-11-20T15:59:34.306936Z","shell.execute_reply":"2023-11-20T15:59:34.523725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Construct KNN model employment.\nknn = KNeighborsClassifier(n_neighbors=7,\n                           weights='distance',     #{distance, uniform}\n                           algorithm = 'auto'\n                           )  ","metadata":{"execution":{"iopub.status.busy":"2023-11-20T15:59:34.526993Z","iopub.execute_input":"2023-11-20T15:59:34.527432Z","iopub.status.idle":"2023-11-20T15:59:34.534769Z","shell.execute_reply.started":"2023-11-20T15:59:34.527396Z","shell.execute_reply":"2023-11-20T15:59:34.533095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply PCA for feature extraction.\nt0 = datetime.datetime.now()\n\npca = PCA(n_components=40)\npca.fit(x_train, y_train)\n\nx_train_pca = pca.transform(x_train)\nx_test_pca = pca.transform(x_test)\nprint(x_train_pca.shape)\nprint(y_train.shape)\nto = datetime.datetime.now() - t0\nprint('PCA runtime:', to)\n# execute the 10-fold cross validation exercise in training dataset.\nt0 = datetime.datetime.now()\n\nCV_accuracy_score = cross_val_score(knn, x_train_pca, y_train, cv=10, scoring = 'accuracy')\nCV_precision_score = cross_val_score(knn,x_train_pca, y_train, cv=10, scoring = 'precision_macro')\nCV_recall_score = cross_val_score(knn, x_train_pca, y_train, cv=10, scoring = 'recall_macro')\nCV_f1_score = cross_val_score(knn, x_train_pca, y_train, cv=10, scoring = 'f1_macro')\naverage_accuracy = np.mean(CV_accuracy_score)\naverage_precision = np.mean(CV_precision_score)\naverage_recall = np.mean(CV_recall_score)\naverage_f1 = np.mean(CV_f1_score)\n\nto = datetime.datetime.now() - t0\nprint('KNN runtime:', to)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T15:59:34.536549Z","iopub.execute_input":"2023-11-20T15:59:34.537028Z","iopub.status.idle":"2023-11-20T16:01:48.931000Z","shell.execute_reply.started":"2023-11-20T15:59:34.536985Z","shell.execute_reply":"2023-11-20T16:01:48.928833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('average_accuracy:', average_accuracy)\nprint('average_precision:', average_precision)\nprint('average_recall:', average_recall)\nprint('average_f1:', average_f1)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T16:01:48.934449Z","iopub.execute_input":"2023-11-20T16:01:48.935369Z","iopub.status.idle":"2023-11-20T16:01:48.941331Z","shell.execute_reply.started":"2023-11-20T16:01:48.935331Z","shell.execute_reply":"2023-11-20T16:01:48.940236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn = knn.fit(x_train_pca, y_train)\npickle.dump(knn,open('model_knn.pkl','wb'))\nknn_pkl = pickle.load(open('model_knn.pkl','rb'))\nknn_pred=knn_pkl.predict(x_test_pca)\nfrom sklearn.preprocessing import label_binarize\n\n# Assuming knn_pred is a 1D array of predicted labels\nknn_pred_multi_label = label_binarize(knn_pred, classes=range(47))\n\nprint(\"Accuracy for KNN {} %\".format(accuracy_score(Y_test, knn_pred_multi_label)*100))\nprint(classification_report(Y_test, knn_pred_multi_label, target_names=labels))","metadata":{"execution":{"iopub.status.busy":"2023-11-20T16:01:48.943000Z","iopub.execute_input":"2023-11-20T16:01:48.943346Z","iopub.status.idle":"2023-11-20T16:01:54.998428Z","shell.execute_reply.started":"2023-11-20T16:01:48.943316Z","shell.execute_reply":"2023-11-20T16:01:54.996624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = np.array(df)\nm, n = data.shape\nnp.random.shuffle(data)\n\ndata_test = data[0:12450].T\nY_test = data_test[n-1]\nX_test = data_test[0:n-1]\nX_test = X_test / 255.\n\ndata_train = data[12450:m].T\nY_train = data_train[n-1]\nX_train = data_train[0:n-1]\nX_train = X_train / 255.\n_,m_train = X_train.shape\n\nprint(_, m_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(Y_train.shape)\nprint(X_test.shape)\nprint(Y_test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train = Y_train.astype(int)\nY_test = Y_test.astype(int)\nprint(Y_train)\nprint(Y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = []\n\nfor i in range(36):\n    \n    if i < 10:\n        classes.append(chr(i+48))\n    else:\n        classes.append(chr(i-10+65))\n        \nclasses","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_params():\n    \n    W1 = np.random.rand(196, 784) - 0.5\n    b1 = np.random.rand(196, 1) - 0.5\n    \n    W2 = np.random.rand(98, 196) - 0.5\n    b2 = np.random.rand(98, 1) - 0.5\n    \n    W3 = np.random.rand(82, 98) - 0.5\n    b3 = np.random.rand(82, 1) - 0.5\n\n    W4 = np.random.rand(36, 82) - 0.5\n    b4 = np.random.rand(36, 1) - 0.5\n    \n    return W1, b1, W2, b2, W3, b3, W4, b4\n\ndef ReLU(Z):\n    return np.maximum(Z, 0)\n\ndef softmax(Z):\n    A = np.exp(Z) / sum(np.exp(Z))\n    return A\n    \ndef forward_prop(W1, b1, W2, b2, W3, b3, W4, b4, X):\n    \n    Z1 = W1.dot(X) + b1\n    A1 = ReLU(Z1)\n    \n    Z2 = W2.dot(A1) + b2\n    A2 = ReLU(Z2)\n    \n    Z3 = W3.dot(A2) + b3\n    A3 = ReLU(Z3)\n    \n    Z4 = W4.dot(A3) + b4\n    A4 = softmax(Z4)\n    \n    return Z1, A1, Z2, A2, Z3, A3, Z4, A4\n\ndef ReLU_deriv(Z):\n    return Z > 0\n\ndef one_hot(Y):\n    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n    one_hot_Y[np.arange(Y.size), Y] = 1\n    one_hot_Y = one_hot_Y.T\n    return one_hot_Y\n\ndef backward_prop(Z1, A1, Z2, A2, Z3, A3, Z4, A4, W1, W2, W3, W4, X, Y):\n\n    one_hot_Y = one_hot(Y)\n\n    dZ4 = A4 - one_hot_Y\n    dW4 = 1 / m * dZ4.dot(A3.T)\n    db4 = 1 / m * np.sum(dZ4)\n    \n    dZ3 = W4.T.dot(dZ4) * ReLU_deriv(Z3)\n    dW3 = 1 / m * dZ3.dot(A2.T)\n    db3 = 1 / m * np.sum(dZ3)\n    \n    dZ2 = W3.T.dot(dZ3) * ReLU_deriv(Z2)\n    dW2 = 1 / m * dZ2.dot(A1.T)\n    db2 = 1 / m * np.sum(dZ2)\n    \n    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n    dW1 = 1 / m * dZ1.dot(X.T)\n    db1 = 1 / m * np.sum(dZ1)\n    \n    return dW1, db1, dW2, db2, dW3, db3, dW4, db4\n\ndef update_params(W1, b1, W2, b2, W3, b3, W4, b4, dW1, db1, dW2, db2, dW3, db3, dW4, db4, alpha):\n    \n    W1 = W1 - alpha * dW1\n    b1 = b1 - alpha * db1    \n    \n    W2 = W2 - alpha * dW2  \n    b2 = b2 - alpha * db2    \n    \n    W3 = W3 - alpha * dW3  \n    b3 = b3 - alpha * db3    \n    \n    W4 = W4 - alpha * dW4  \n    b4 = b4 - alpha * db4        \n    \n    return W1, b1, W2, b2, W3, b3, W4, b4","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_predictions(A4):\n    return np.argmax(A4, 0)\n\ndef get_accuracy(predictions, Y):\n    print(predictions, Y)\n    return np.sum(predictions == Y) / Y.size\n\ndef gradient_descent(X, Y, alpha, iterations):\n    W1, b1, W2, b2, W3, b3, W4, b4 = init_params()\n    for i in range(iterations):\n        Z1, A1, Z2, A2, Z3, A3, Z4, A4 = forward_prop(W1, b1, W2, b2, W3, b3, W4, b4, X)\n        dW1, db1, dW2, db2, dW3, db3, dW4, db4 = backward_prop(Z1, A1, Z2, A2, Z3, A3, Z4, A4, W1, W2, W3, W4, X, Y)\n        W1, b1, W2, b2, W3, b3, W4, b4 = update_params(W1, b1, W2, b2, W3, b3, W4, b4, dW1, db1, dW2, db2, dW3, db3, dW4, db4, alpha)\n        if i % 50 == 0:\n#             print(\"Iteration: \", i)\n            predictions = get_predictions(A4)\n            print(\"Accuracy : \", get_accuracy(predictions, Y))\n            \n            Z1, A1, Z2, A2, Z3, A3, Z4, A4 = forward_prop(W1, b1, W2, b2, W3, b3, W4, b4, X_test)\n            predictions = get_predictions(A4)\n            print(\"Validation Accuracy : \", get_accuracy(predictions, Y_test))\n            \n            print('\\n\\n')\n    return W1, b1, W2, b2, W3, b3, W4, b4","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"W1, b1, W2, b2, W3, b3, W4, b4 = gradient_descent(X_train, Y_train, 0.01, 2500)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_predictions(X, W1, b1, W2, b2, W3, b3, W4, b4):\n    Z1, A1, Z2, A2, Z3, A3, Z4, A4 = forward_prop(W1, b1, W2, b2, W3, b3, W4, b4, X)\n    predictions = get_predictions(A4)\n    return predictions\n\ndef test_prediction(index, W1, b1, W2, b2, W3, b3, W4, b4):\n    current_image = X_train[:, index, None]\n    prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2, W3, b3, W4, b4)\n    label = Y_train[index]\n    print(\"Prediction: \", prediction)\n    print(\"Label: \", label)\n    \n    current_image = current_image.reshape((28, 28)) * 255\n    plt.gray()\n    plt.imshow(current_image, interpolation='nearest')\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_prediction(1, W1, b1, W2, b2, W3, b3, W4, b4, W5, b5)\ntest_prediction(2, W1, b1, W2, b2, W3, b3, W4, b4, W5, b5)\ntest_prediction(3, W1, b1, W2, b2, W3, b3, W4, b4, W5, b5)\ntest_prediction(4, W1, b1, W2, b2, W3, b3, W4, b4, W5, b5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions = make_predictions(X_test, W1, b1, W2, b2, W3, b3, W4, b4, W5, b5)\nget_accuracy(dev_predictions, Y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ANN","metadata":{}},{"cell_type":"code","source":"X_train = X_train.T\nX_train.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = X_test.T\nX_test.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_test.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import BatchNormalization, Activation, Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n# from tensorflow.keras.optimizers import Adam\n# from tensorflow.keras.metrics import categorical _crossentropy\nfrom keras.utils import to_categorical\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport itertools","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train_encoded = to_categorical(Y_train, num_classes=36)\nY_test_encoded = to_categorical(Y_test, num_classes=36)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann = Sequential([\n    Dense(784, activation='relu', input_shape=(784,)),\n    Dropout(0.5),\n    Dense(512, activation='relu'),\n    Dropout(0.5),\n    Dense(256, activation='relu'),\n    Dropout(0.5),\n    Dense(36, activation='softmax')\n])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann_fit=ann.fit(X_train, Y_train_encoded, epochs=10, validation_data=(X_test,Y_test_encoded))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = ann_fit.history\n\ntraining_loss = history['loss']\nvalidation_loss = history['val_loss']\n\ntraining_accuracy = history['accuracy']\nvalidation_accuracy = history['val_accuracy']\n\n\nprint(\"Training Loss:\", training_loss)\nprint(\"Validation Loss:\", validation_loss)\nprint(\"Training Accuracy:\", training_accuracy)\nprint(\"Validation Accuracy:\", validation_accuracy)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the Loss\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(training_loss, label='Training Loss')\nplt.plot(validation_loss, label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plotting the Accuracy\nplt.subplot(1, 2, 2)\nplt.plot(training_accuracy, label='Training Accuracy')\nplt.plot(validation_accuracy, label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = ann.predict(X_test)\npredictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_test_pred = np.argmax(predictions, axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cls_report = classification_report(Y_test, Y_test_pred)\nprint(cls_report)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_true = Y_test, y_pred = Y_test_pred)\ncm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_matrix(cm, classes, cmap=plt.cm.Blues):\n    \n    plt.figure(figsize=(23,23))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title('Confusion matrix')\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(cm, classes)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN","metadata":{}},{"cell_type":"code","source":"X_train_reshaped = X_train.reshape(-1, 28, 28, 1)\nX_test_reshaped = X_test.reshape(-1, 28, 28, 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn = Sequential([\n    \n    Conv2D(32, kernel_size=(3, 3), input_shape=(28, 28, 1), padding='same'),\n    BatchNormalization(),\n    Activation('relu'),\n    MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'),\n    \n    Conv2D(64, kernel_size=(3, 3), padding='same'),\n    BatchNormalization(),\n    Activation('relu'),\n    MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'),\n    \n    Flatten(),\n    \n    Dense(128, activation='relu'),\n#     Dropout(0.5),\n    Dense(64, activation='relu'),\n#     Dropout(0.5),\n    Dense(36, activation='softmax')\n])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_fit=cnn.fit(X_train_reshaped, Y_train_encoded, epochs=10, validation_data=(X_test_reshaped,Y_test_encoded))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = cnn_fit.history\n\ntraining_loss = history['loss']\nvalidation_loss = history['val_loss']\n\ntraining_accuracy = history['accuracy']\nvalidation_accuracy = history['val_accuracy']\n\n\nprint(\"Training Loss:\", training_loss)\nprint(\"Validation Loss:\", validation_loss)\nprint(\"Training Accuracy:\", training_accuracy)\nprint(\"Validation Accuracy:\", validation_accuracy)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the Loss\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(training_loss, label='Training Loss')\nplt.plot(validation_loss, label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plotting the Accuracy\nplt.subplot(1, 2, 2)\nplt.plot(training_accuracy, label='Training Accuracy')\nplt.plot(validation_accuracy, label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = cnn.predict(X_test)\npredictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_test_pred = np.argmax(Y_pred, axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cls_report = classification_report(Y_test, Y_test_pred)\ncls_report","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_true = Y_test, y_pred = Y_test_pred)\ncm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(cm, classes)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}